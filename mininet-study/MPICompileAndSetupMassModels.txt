





#MPI_Delay_test iperf result:
#Compile: Only on one machine
java -DPART_STR="2::1:1,2:1" -DRUNTIME_ENV="[c_slave1,d_slave1,eth2,10.10.1.1],[c_slave2,d_slave2,eth3,10.10.2.1]" -DPORTAL_LINKS="c_slave1:eth2,top.left.r1.portal_if_10_10_1_0,c_slave2:eth3,top.right.r2.portal_if_10_10_2_0" -jar ~/primogeni_mpi/netscript/dist/jprime.jar create PortalMPI_LinkBW1gbpsDelay10microSec PortalMPI_LinkBW1gbpsDelay10microSec.java
java -DPART_STR="2::1:1,2:1" -DRUNTIME_ENV="[c_slave1,d_slave1,eth2,10.10.1.1],[c_slave2,d_slave2,eth3,10.10.2.1]" -DPORTAL_LINKS="c_slave1:eth2,top.left.r1.portal_if_10_10_1_0,c_slave2:eth3,top.right.r2.portal_if_10_10_2_0" -jar ~/primogeni_mpi/netscript/dist/jprime.jar create PortalMPI_LinkBW1gbpsDelay100microSec PortalMPI_LinkBW1gbpsDelay100microSec.java
java -DPART_STR="2::1:1,2:1" -DRUNTIME_ENV="[c_slave1,d_slave1,eth2,10.10.1.1],[c_slave2,d_slave2,eth3,10.10.2.1]" -DPORTAL_LINKS="c_slave1:eth2,top.left.r1.portal_if_10_10_1_0,c_slave2:eth3,top.right.r2.portal_if_10_10_2_0" -jar ~/primogeni_mpi/netscript/dist/jprime.jar create PortalMPI_LinkBW1gbpsDelay1ms PortalMPI_LinkBW1gbpsDelay1ms.java
java -DPART_STR="2::1:1,2:1" -DRUNTIME_ENV="[c_slave1,d_slave1,eth2,10.10.1.1],[c_slave2,d_slave2,eth3,10.10.2.1]" -DPORTAL_LINKS="c_slave1:eth2,top.left.r1.portal_if_10_10_1_0,c_slave2:eth3,top.right.r2.portal_if_10_10_2_0" -jar ~/primogeni_mpi/netscript/dist/jprime.jar create PortalMPI_LinkBW1gbpsDelay10ms PortalMPI_LinkBW1gbpsDelay10ms.java
java -DPART_STR="2::1:1,2:1" -DRUNTIME_ENV="[c_slave1,d_slave1,eth2,10.10.1.1],[c_slave2,d_slave2,eth3,10.10.2.1]" -DPORTAL_LINKS="c_slave1:eth2,top.left.r1.portal_if_10_10_1_0,c_slave2:eth3,top.right.r2.portal_if_10_10_2_0" -jar ~/primogeni_mpi/netscript/dist/jprime.jar create PortalMPI_LinkBW1gbpsDelay100ms PortalMPI_LinkBW1gbpsDelay100ms.java


#PC1(Pollux) 10.1.0.28
sudo ip link add h9-eth0 type veth peer name pollux-eth9
sleep 1
sudo ip netns add h9
sudo ip link set h9-eth0 netns h9
sudo ip netns exec h9 ifconfig h9-eth0 10.10.1.2/24
sudo ifconfig pollux-eth9 10.10.1.1/24
sudo ip netns exec h9 ifconfig lo up
sudo ip netns exec h9 ping -c1 10.10.1.1
sudo  ping -c1 10.10.1.2
#Container Routing:
sudo ip netns exec h9 ip route add 192.168.0.0/16 via 10.10.1.1
sudo ip netns exec h9 ip route add 10.10.2.0/24 via 10.10.1.1


#PC2 (or, Castor)   --10.1.0.29
sudo ip link add h10-eth0 type veth peer name castor-eth10
sleep 1
sudo ip netns add h10
sudo ip link set h10-eth0 netns h10
sudo ip netns exec h10 ifconfig h10-eth0 10.10.2.2/24
sudo ifconfig castor-eth10 10.10.2.1/24
sudo ip netns exec h10 ifconfig lo up
sudo ip netns exec h10 ping -c1 10.10.2.1
sudo  ping -c1 10.10.2.2
#Container Routing:
sudo ip netns exec h10 ip route add 192.168.0.0/16 via 10.10.2.1
sudo ip netns exec h10 ip route add 10.10.1.0/24 via 10.10.2.1


#As long as the pcs are in local network we can run MPi models. all we need is machineFile


#**************************************************************************
#			Initiating monitor.sh in slave machines(pollux,castor)
#**************************************************************************
sh /root/primogeni_mpi/monitor.sh &
#SO that mina listens to 9992 port for MPI calls


#**************************************************************************
#			MPI Running 1 : 1gb, default delay
#**************************************************************************

java -DPART_STR="2::1:1,2:1" -DRUNTIME_ENV="[c_slave1,d_slave1,eth2,10.10.1.1],[c_slave2,d_slave2,eth3,10.10.2.1]" -DPORTAL_LINKS="c_slave1:eth2,top.left.r1.portal_if_10_10_1_0,c_slave2:eth3,top.right.r2.portal_if_10_10_2_0" -jar ~/primogeni_mpi/netscript/dist/jprime.jar create PortalMPI_default_parameters2 PortalMPI_default_parameters2.java

scp PortalMPI_default_parameters2_part_* root@castor:~/expt/mpi

Make sure monitor is running

mpiexec -f /root/primogeni_mpi/machineFile -n 1 /root/primogeni_mpi/netsim/primex -tp 10.10.1.1 86  -enable_state_stream 600 /root/expt/mpi/PortalMPI_default_parameters2_part_2.tlv : -n 1 /root/primogeni_mpi/netsim/primex -tp 10.10.2.1 221  -enable_state_stream 600 /root/expt/mpi/PortalMPI_default_parameters2_part_1.tlv -v /root/expt/mpi/runtime_symbols.txt

#pollux# 
sudo ip netns exec h1 ping 192.168.0.1
#castor# 
sudo ip netns exec h2 ping 192.168.0.1

10.10.1.2(h1)    <--------->         (h1)10.10.2.2
pollux:
sudo ip netns exec h1 iperf -s -w 16M -i 1
castor:
sudo ip netns exec h2 iperf -c 10.10.1.2 -w 16M

we got
[  6]  0.0-10.0 sec   156 MBytes   131 Mbits/sec


#**************************************************************************
#			MPI Running 2: PortalMPI_1Gb10ms.java
#**************************************************************************

java -DPART_STR="2::1:1,2:1" -DRUNTIME_ENV="[c_slave1,d_slave1,eth2,10.10.1.1],[c_slave2,d_slave2,eth3,10.10.2.1]" -DPORTAL_LINKS="c_slave1:eth2,top.left.r1.portal_if_10_10_1_0,c_slave2:eth3,top.right.r2.portal_if_10_10_2_0" -jar ~/primogeni_mpi/netscript/dist/jprime.jar create PortalMPI_1Gb10ms PortalMPI_1Gb10ms.java

scp PortalMPI_1Gb10ms* root@castor:~/expt/mpi

Make sure monitor is running

mpiexec -f /root/primogeni_mpi/machineFile -n 1 /root/primogeni_mpi/netsim/primex -tp 10.10.1.1 86  -enable_state_stream 600 /root/expt/mpi/PortalMPI_1Gb10ms_part_2.tlv : -n 1 /root/primogeni_mpi/netsim/primex -tp 10.10.2.1 221  -enable_state_stream 600 /root/expt/mpi/PortalMPI_1Gb10ms_part_1.tlv -v /root/expt/mpi/runtime_symbols.txt

#pollux# 
sudo ip netns exec h1 ping 192.168.0.1
#castor# 
sudo ip netns exec h2 ping 192.168.0.1

10.10.1.2(h1)    <--------->         (h1)10.10.2.2
pollux:
sudo ip netns exec h1 iperf -s -w 16M -i 1
castor:
sudo ip netns exec h2 iperf -c 10.10.1.2 -w 16M
We got;
[  6]  0.0-10.1 sec  32.8 MBytes  27.1 Mbits/sec
[  6]  0.0-10.1 sec  32.8 MBytes  27.1 Mbits/sec
[  6]  0.0-10.1 sec  30.4 MBytes  25.1 Mbits/sec





#**************************************************************************
#			MPI Running 3: PortalMPI_1Gb100ms.java
#**************************************************************************

java -DPART_STR="2::1:1,2:1" -DRUNTIME_ENV="[c_slave1,d_slave1,eth2,10.10.1.1],[c_slave2,d_slave2,eth3,10.10.2.1]" -DPORTAL_LINKS="c_slave1:eth2,top.left.r1.portal_if_10_10_1_0,c_slave2:eth3,top.right.r2.portal_if_10_10_2_0" -jar ~/primogeni_mpi/netscript/dist/jprime.jar create PortalMPI_1Gb100ms PortalMPI_1Gb100ms.java

scp PortalMPI_1Gb100ms* root@castor:~/expt/mpi

Make sure monitor is running

mpiexec -f /root/primogeni_mpi/machineFile -n 1 /root/primogeni_mpi/netsim/primex -tp 10.10.1.1 86  -enable_state_stream 600 /root/expt/mpi/PortalMPI_1Gb100ms_part_2.tlv : -n 1 /root/primogeni_mpi/netsim/primex -tp 10.10.2.1 221  -enable_state_stream 600 /root/expt/mpi/PortalMPI_1Gb100ms_part_1.tlv -v /root/expt/mpi/runtime_symbols.txt

#pollux# 
sudo ip netns exec h1 ping 192.168.0.1
#castor# 
sudo ip netns exec h2 ping 192.168.0.1

10.10.1.2(h1)    <--------->         (h1)10.10.2.2
pollux:
sudo ip netns exec h1 iperf -s -w 16M -i 1
castor:
sudo ip netns exec h2 iperf -c 10.10.1.2 -w 16M

we got 
[  7]  0.0-11.3 sec  2.12 MBytes  1.58 Mbits/sec






#**************************************************************************
#			MPI Running 4: PortalMPI_1Gb500micros
#**************************************************************************

java -DPART_STR="2::1:1,2:1" -DRUNTIME_ENV="[c_slave1,d_slave1,eth2,10.10.1.1],[c_slave2,d_slave2,eth3,10.10.2.1]" -DPORTAL_LINKS="c_slave1:eth2,top.left.r1.portal_if_10_10_1_0,c_slave2:eth3,top.right.r2.portal_if_10_10_2_0" -jar ~/primogeni_mpi/netscript/dist/jprime.jar create PortalMPI_1Gb500micros PortalMPI_1Gb500micros.java

scp PortalMPI_1Gb500micros* root@castor:~/expt/mpi

mpiexec -f /root/primogeni_mpi/machineFile -n 1 /root/primogeni_mpi/netsim/primex -tp 10.10.1.1 86  -enable_state_stream 600 /root/expt/mpi/PortalMPI_1Gb500micros_part_2.tlv : -n 1 /root/primogeni_mpi/netsim/primex -tp 10.10.2.1 221  -enable_state_stream 600 /root/expt/mpi/PortalMPI_1Gb500micros_part_1.tlv -v /root/expt/mpi/runtime_symbols.txt

#10.10.1.2(h1)    <--------->         (h1)10.10.2.2
#pollux:
sudo ip netns exec h1 iperf -s -w 16M -i 1
#castor:
sudo ip netns exec h2 iperf -c 10.10.1.2 -w 16M

#we got 
[  6]  0.0-10.0 sec   233 MBytes   195 Mbits/sec
[  7]  0.0-10.0 sec   219 MBytes   183 Mbits/sec
[  6]  0.0-10.0 sec   228 MBytes   191 Mbits/sec
[  7]  0.0-10.0 sec   197 MBytes   165 Mbits/sec (No Iperf Cleanup)
[  6]  0.0-10.0 sec   200 MBytes   168 Mbits/sec (No Iperf Cleanup)
[  6]  0.0-10.0 sec   224 MBytes   188 Mbits/sec
[  7]  0.0-10.0 sec   234 MBytes   197 Mbits/sec


#**************************************************************************
#			MPI Running 5: PortalMPI_1Gb250micros
#**************************************************************************

java -DPART_STR="2::1:1,2:1" -DRUNTIME_ENV="[c_slave1,d_slave1,eth2,10.10.1.1],[c_slave2,d_slave2,eth3,10.10.2.1]" -DPORTAL_LINKS="c_slave1:eth2,top.left.r1.portal_if_10_10_1_0,c_slave2:eth3,top.right.r2.portal_if_10_10_2_0" -jar ~/primogeni_mpi/netscript/dist/jprime.jar create PortalMPI_1Gb250micros PortalMPI_1Gb250micros.java

scp PortalMPI_1Gb250micros* root@castor:~/expt/mpi

mpiexec -f /root/primogeni_mpi/machineFile -n 1 /root/primogeni_mpi/netsim/primex -tp 10.10.1.1 86  -enable_state_stream 600 /root/expt/mpi/PortalMPI_1Gb250micros_part_2.tlv : -n 1 /root/primogeni_mpi/netsim/primex -tp 10.10.2.1 221  -enable_state_stream 600 /root/expt/mpi/PortalMPI_1Gb250micros_part_1.tlv -v /root/expt/mpi/runtime_symbols.txt

#pollux# 
sudo ip netns exec h1 ping 192.168.0.1
#castor# 
sudo ip netns exec h2 ping 192.168.0.1

10.10.1.2(h1)    <--------->         (h1)10.10.2.2
pollux:
sudo ip netns exec h1 iperf -s -w 16M -i 1
castor:
sudo ip netns exec h2 iperf -c 10.10.1.2 -w 16M

we got 
[  6]  0.0-10.0 sec   212 MBytes   178 Mbits/sec
[  7]  0.0-10.0 sec   217 MBytes   181 Mbits/sec
[  6]  0.0-10.0 sec   212 MBytes   178 Mbits/sec
[  6]  0.0-10.0 sec   211 MBytes   177 Mbits/sec
[  7]  0.0-10.0 sec   213 MBytes   178 Mbits/sec
[  6]  0.0-10.0 sec   213 MBytes   179 Mbits/sec




#**************************************************************************
#			MPI Running 5: PortalMPI_1Gb125micros
#**************************************************************************

java -DPART_STR="2::1:1,2:1" -DRUNTIME_ENV="[c_slave1,d_slave1,eth2,10.10.1.1],[c_slave2,d_slave2,eth3,10.10.2.1]" -DPORTAL_LINKS="c_slave1:eth2,top.left.r1.portal_if_10_10_1_0,c_slave2:eth3,top.right.r2.portal_if_10_10_2_0" -jar ~/primogeni_mpi/netscript/dist/jprime.jar create PortalMPI_1Gb125micros PortalMPI_1Gb125micros.java

scp PortalMPI_1Gb125micros* root@castor:~/expt/mpi

mpiexec -f /root/primogeni_mpi/machineFile -n 1 /root/primogeni_mpi/netsim/primex -tp 10.10.1.1 86  -enable_state_stream 600 /root/expt/mpi/PortalMPI_1Gb125micros_part_2.tlv : -n 1 /root/primogeni_mpi/netsim/primex -tp 10.10.2.1 221  -enable_state_stream 600 /root/expt/mpi/PortalMPI_1Gb125micros_part_1.tlv -v /root/expt/mpi/runtime_symbols.txt

#pollux# 
sudo ip netns exec h1 ping 192.168.0.1
#castor# 
sudo ip netns exec h2 ping 192.168.0.1

10.10.1.2(h1)    <--------->         (h1)10.10.2.2
pollux:
sudo ip netns exec h1 iperf -s -w 16M -i 1
castor:
sudo ip netns exec h2 iperf -c 10.10.1.2 -w 16M

we got 
[  6]  0.0-11.5 sec  4.75 MBytes  3.46 Mbits/sec
[  7]  0.0-11.8 sec  2.75 MBytes  1.95 Mbits/sec
[  6]  0.0-14.7 sec  4.38 MBytes  2.50 Mbits/sec
[  6]  0.0-10.7 sec  5.75 MBytes  4.51 Mbits/sec
[  7]  0.0-10.2 sec  5.62 MBytes  4.62 Mbits/sec



#**************************************************************************
#			MPI Running 6: PortalMPI_1Gb0delay
#**************************************************************************

java -DPART_STR="2::1:1,2:1" -DRUNTIME_ENV="[c_slave1,d_slave1,eth2,10.10.1.1],[c_slave2,d_slave2,eth3,10.10.2.1]" -DPORTAL_LINKS="c_slave1:eth2,top.left.r1.portal_if_10_10_1_0,c_slave2:eth3,top.right.r2.portal_if_10_10_2_0" -jar ~/primogeni_mpi/netscript/dist/jprime.jar create PortalMPI_1Gb0delay PortalMPI_1Gb0delay.java

scp PortalMPI_1Gb0delay* root@castor:~/expt/mpi

mpiexec -f /root/primogeni_mpi/machineFile -n 1 /root/primogeni_mpi/netsim/primex -tp 10.10.1.1 86  -enable_state_stream 600 /root/expt/mpi/PortalMPI_1Gb0delay_part_2.tlv : -n 1 /root/primogeni_mpi/netsim/primex -tp 10.10.2.1 221  -enable_state_stream 600 /root/expt/mpi/PortalMPI_1Gb0delay_part_1.tlv -v /root/expt/mpi/runtime_symbols.txt

we got:

ERROR: ssf kernel: cross-timeline channels must have positive delays
ERROR: ssf kernel: cross-timeline channels must have positive delays
[mpiexec@pollux] ONE OF THE PROCESSES TERMINATED BADLY: CLEANING UP
[proxy:0:0@pollux] HYD_pmcd_pmip_control_cmd_cb (./pm/pmiserv/pmip_cb.c:868): assert (!closed) failed
[proxy:0:0@pollux] HYDT_dmxu_poll_wait_for_event (./tools/demux/demux_poll.c:77): callback returned error status
[proxy:0:0@pollux] main (./pm/pmiserv/pmip.c:208): demux engine error waiting for event
[mpiexec@pollux] HYDT_bscu_wait_for_completion (./tools/bootstrap/utils/bscu_wait.c:70): one of the processes terminated badly; aborting
[mpiexec@pollux] HYDT_bsci_wait_for_completion (./tools/bootstrap/src/bsci_wait.c:18): launcher returned error waiting for completion
[mpiexec@pollux] HYD_pmci_wait_for_completion (./pm/pmiserv/pmiserv_pmci.c:216): launcher returned error waiting for completion
[mpiexec@pollux] main (./ui/mpich/mpiexec.c:404): process manager error waiting for completion
